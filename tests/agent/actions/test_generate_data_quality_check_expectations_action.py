from __future__ import annotations

import logging
import uuid
from typing import TYPE_CHECKING, Any

import great_expectations.expectations as gx_expectations
import pytest
from great_expectations.datasource.fluent.sql_datasource import TableAsset
from great_expectations.expectations.metadata_types import DataQualityIssues
from great_expectations.experimental.metric_repository.batch_inspector import (
    BatchInspector,
)
from great_expectations.experimental.metric_repository.metric_repository import (
    MetricRepository,
)
from great_expectations.experimental.metric_repository.metrics import (
    MetricRun,
    MetricTypes,
    TableMetric,
)

from great_expectations_cloud.agent.actions.generate_data_quality_check_expectations_action import (
    GenerateDataQualityCheckExpectationsAction,
    PartialGenerateDataQualityCheckExpectationError,
)
from great_expectations_cloud.agent.models import GenerateDataQualityCheckExpectationsEvent

if TYPE_CHECKING:
    from great_expectations.datasource.fluent import DataAsset
    from pytest_mock import MockerFixture

pytestmark = pytest.mark.unit

LOGGER = logging.getLogger(__name__)


@pytest.fixture
def mock_metrics_list() -> list[TableMetric[Any]]:
    return [
        TableMetric(
            batch_id="batch_id",
            metric_name="table.columns",
            value=["col1", "col2"],
            exception=None,
        ),
        TableMetric(
            batch_id="batch_id",
            metric_name="table.column_types",
            value=[
                {"name": "col1", "type": "INT"},
                {"name": "col2", "type": "INT"},
            ],
            exception=None,
        ),
    ]


TABLE_ASSET_ID = uuid.uuid4()


# https://docs.pytest.org/en/7.1.x/how-to/monkeypatch.html
@pytest.fixture
def mock_response_success(monkeypatch, mock_metrics_list: list[TableMetric[Any]]):
    def mock_data_asset(self, event: GenerateDataQualityCheckExpectationsEvent, asset_name: str):
        return TableAsset(
            id=TABLE_ASSET_ID,
            name="test-data-asset",
            table_name="test_table",
            schema_name="test_schema",
        )

    def mock_metrics(self, data_asset: DataAsset[Any, Any]):
        return MetricRun(metrics=mock_metrics_list), uuid.uuid4()

    monkeypatch.setattr(
        GenerateDataQualityCheckExpectationsAction,
        "_retrieve_asset_from_asset_name",
        mock_data_asset,
    )
    monkeypatch.setattr(GenerateDataQualityCheckExpectationsAction, "_get_metrics", mock_metrics)


@pytest.fixture
def mock_multi_asset_success_and_failure(monkeypatch, mock_metrics_list: list[TableMetric[Any]]):
    failing_asset_id = uuid.UUID("00000000-0000-0000-0000-000000000001")

    def mock_data_asset(self, event: GenerateDataQualityCheckExpectationsEvent, asset_name: str):
        if "retrieve-fail" in asset_name:
            raise RuntimeError(f"Failed to retrieve asset: {asset_name}")  # noqa: TRY003 # following pattern in code
        elif "schema-fail" in asset_name:
            return TableAsset(
                id=failing_asset_id,
                name=asset_name,
                table_name="test_table",
                schema_name="test_schema",
            )
        else:
            return TableAsset(
                name=asset_name,
                table_name="test_table",
                schema_name="test_schema",
            )

    def mock_metrics(self, data_asset: DataAsset[Any, Any]):
        if "metric-fail" in data_asset.name:
            raise RuntimeError("One or more metrics failed to compute.")  # noqa: TRY003 # following pattern in code
        else:
            return MetricRun(metrics=mock_metrics_list)

    def mock_schema_change_expectation(self, metric_run: MetricRun, asset_id: uuid.UUID):
        # The data asset name is contained in the expectation_suite_name
        # Here we are simulating a failure to add an expectation to the suite, for suite names that contain "schema-fail"
        if asset_id == failing_asset_id:
            error_msg = "Failed to add autogenerated expectation: expect_table_columns_to_match_set"
            raise RuntimeError(error_msg)
        else:
            return uuid.uuid4()

    monkeypatch.setattr(
        GenerateDataQualityCheckExpectationsAction,
        "_retrieve_asset_from_asset_name",
        mock_data_asset,
    )
    monkeypatch.setattr(GenerateDataQualityCheckExpectationsAction, "_get_metrics", mock_metrics)
    monkeypatch.setattr(
        GenerateDataQualityCheckExpectationsAction,
        "_add_schema_change_expectation",
        mock_schema_change_expectation,
    )


@pytest.mark.parametrize(
    "data_asset_names, expected_created_resources",
    [
        (["test-data-asset1"], 2),
        (
            ["test-data-asset1", "test-data-asset2"],
            4,
        ),
    ],
)
def test_generate_schema_change_expectations_action_success(
    mock_response_success,
    mock_context,
    mocker: MockerFixture,
    data_asset_names,
    expected_created_resources,
):
    # setup
    mock_metric_repository = mocker.Mock(spec=MetricRepository)
    mock_batch_inspector = mocker.Mock(spec=BatchInspector)

    action = GenerateDataQualityCheckExpectationsAction(
        context=mock_context,
        metric_repository=mock_metric_repository,
        batch_inspector=mock_batch_inspector,
        base_url="",
        auth_key="",
        organization_id=uuid.uuid4(),
    )

    # run the action
    mocker.patch(
        f"{GenerateDataQualityCheckExpectationsAction.__module__}.{GenerateDataQualityCheckExpectationsAction.__name__}._create_expectation_for_asset",
        return_value=uuid.uuid4(),
    )
    mock_create_expectation_for_asset = mocker.spy(
        GenerateDataQualityCheckExpectationsAction, "_create_expectation_for_asset"
    )
    return_value = action.run(
        event=GenerateDataQualityCheckExpectationsEvent(
            type="generate_data_quality_check_expectations_request.received",
            organization_id=uuid.uuid4(),
            datasource_name="test-datasource",
            data_assets=data_asset_names,
            selected_data_quality_issues=[DataQualityIssues.SCHEMA],
        ),
        id="test-id",
    )

    # assert
    assert len(return_value.created_resources) == expected_created_resources
    assert return_value.type == "generate_data_quality_check_expectations_request.received"
    mock_create_expectation_for_asset.assert_called()
    mock_create_expectation_for_asset.assert_called_with(
        expectation=gx_expectations.ExpectTableColumnsToMatchSet(
            column_set=["col1", "col2"],
        ),
        asset_id=TABLE_ASSET_ID,
    )


@pytest.mark.parametrize(
    "succeeding_data_asset_names, failing_data_asset_names, error_message_header",
    [
        pytest.param(
            ["test-data-asset1"],
            ["retrieve-fail-asset-1"],
            "Unable to autogenerate expectations for 1 of 2 Data Assets.",
            id="Single asset passing, single asset failing",
        ),
        pytest.param(
            ["test-data-asset1", "test-data-asset2"],
            ["retrieve-fail-asset-1"],
            "Unable to autogenerate expectations for 1 of 3 Data Assets.",
            id="Multiple assets passing, single asset failing",
        ),
        pytest.param(
            ["test-data-asset1", "test-data-asset2"],
            [
                "retrieve-fail-asset-1",
                "retrieve-fail-asset-2",
                "metric-fail-asset-1",
                "metric-fail-asset-2",
            ],
            "Unable to autogenerate expectations for 4 of 6 Data Assets.",
            id="Multiple assets passing, multiple assets failing",
        ),
        pytest.param(
            ["test-data-asset1", "test-data-asset2"],
            [
                "retrieve-fail-asset-1",
                "retrieve-fail-asset-2",
                "metric-fail-asset-1",
                "metric-fail-asset-2",
                "schema-fail-asset-1",
                "schema-fail-asset-2",
                "schema-fail-asset-3",
                "schema-fail-asset-4",
                "schema-fail-asset-5",
            ],
            "Unable to autogenerate expectations for 9 of 11 Data Assets.",
            id="Multiple assets passing, multiple assets failing, more than max display errors",
        ),
    ],
)
def test_succeeding_and_failing_assets_together(
    mock_multi_asset_success_and_failure,
    mock_context,
    mocker: MockerFixture,
    succeeding_data_asset_names: list[str],
    failing_data_asset_names: list[str],
    error_message_header: str,
):
    # setup
    mock_metric_repository = mocker.Mock(spec=MetricRepository)
    mock_batch_inspector = mocker.Mock(spec=BatchInspector)

    action = GenerateDataQualityCheckExpectationsAction(
        context=mock_context,
        metric_repository=mock_metric_repository,
        batch_inspector=mock_batch_inspector,
        base_url="",
        auth_key="",
        organization_id=uuid.uuid4(),
    )

    # run the action
    with pytest.raises(PartialGenerateDataQualityCheckExpectationError) as e:
        action.run(
            event=GenerateDataQualityCheckExpectationsEvent(
                type="generate_data_quality_check_expectations_request.received",
                organization_id=uuid.uuid4(),
                datasource_name="test-datasource",
                data_assets=succeeding_data_asset_names + failing_data_asset_names,
                selected_data_quality_issues=[DataQualityIssues.SCHEMA],
            ),
            id="test-id",
        )

    # These are part of the same message
    assert error_message_header in str(e.value)
    error_message_footer = "Check your connection details, delete and recreate these Data Assets."
    assert error_message_footer in str(e.value)
    for asset_name in failing_data_asset_names:
        assert asset_name in str(e.value)


def test_missing_table_columns_metric_raises_runtime_error(
    mock_context,
    mocker: MockerFixture,
):
    # Setup
    mock_metric_repository = mocker.Mock(spec=MetricRepository)
    mock_batch_inspector = mocker.Mock(spec=BatchInspector)

    # Create a mock data asset
    mock_data_asset = mocker.Mock()
    mock_data_asset.id = uuid.uuid4()
    mock_data_asset.test_connection.return_value = None

    # Mock the context to return our mock data asset
    mock_context.data_sources.get.return_value.get_asset.return_value = mock_data_asset

    # Create a metric run without TABLE_COLUMNS metric
    mock_metric_run = mocker.Mock(spec=MetricRun)
    mock_metric_run.metrics = [
        # No TABLE_COLUMNS metric in this list
        TableMetric(
            batch_id="batch_id",
            metric_name=MetricTypes.TABLE_ROW_COUNT,
            value=100,
            exception=None,
        )
    ]

    # Configure the batch inspector to return our mock metric run
    mock_batch_inspector.compute_metric_list_run.return_value = mock_metric_run
    mock_metric_repository.add_metric_run.return_value = uuid.uuid4()

    action = GenerateDataQualityCheckExpectationsAction(
        context=mock_context,
        metric_repository=mock_metric_repository,
        batch_inspector=mock_batch_inspector,
        base_url="",
        auth_key="",
        organization_id=uuid.uuid4(),
    )

    # Create the event
    event = GenerateDataQualityCheckExpectationsEvent(
        type="generate_data_quality_check_expectations_request.received",
        organization_id=uuid.uuid4(),
        datasource_name="test-datasource",
        data_assets=["test-asset"],
        selected_data_quality_issues=[DataQualityIssues.SCHEMA],
    )

    # Act & Assert
    with pytest.raises(PartialGenerateDataQualityCheckExpectationError):
        action.run(event=event, id="test-id")


if __name__ == "__main__":
    print(GenerateDataQualityCheckExpectationsEvent.__module__)
